{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import ne_chunk\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Analysis of Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NLP_My_Words():\n",
    "    \"\"\"Tokenize and analize text\"\"\"\n",
    "    def __init__(self,text,case='y'):\n",
    "        \n",
    "        if (case!='y'):\n",
    "            self.text=text\n",
    "        else:\n",
    "            self.text=text.lower()\n",
    "        \n",
    "        raw_word_count=len(self.text.split())\n",
    "        print(f'Raw text word count: {raw_word_count}\\n')\n",
    "        self.tokenization()\n",
    "        \n",
    "    def tokenization(self):\n",
    "        \"\"\"Tokenization\"\"\"\n",
    "        self.tokens = nltk.word_tokenize(self.text)\n",
    "        punctuation=re.compile(r'[-.?!,:;()|=/*\"><\\{}$&]')\n",
    "\n",
    "        post_punctuation=[]\n",
    "        for words in self.tokens:\n",
    "            word=punctuation.sub('',words)\n",
    "            if len(word)>0:\n",
    "                post_punctuation.append(word)\n",
    "\n",
    "        self.tokens=post_punctuation\n",
    "\n",
    "        self.nltk_text=nltk.Text(self.tokens)\n",
    "        \n",
    "        self.remove_stop_words()\n",
    "\n",
    "\n",
    "    def sentenance_tokenization(self):\n",
    "        \"\"\"Sentenance Tokenization\"\"\"\n",
    "    \n",
    "        self.sentenance_tokens=sent_tokenize(self.text)\n",
    "\n",
    "        return self.sentenance_tokens\n",
    "    \n",
    "    \n",
    "    def word_freq(self):\n",
    "        \"\"\"Word Frequency\"\"\"\n",
    "    \n",
    "        freq = nltk.FreqDist(self.tokens)\n",
    "\n",
    "        for key,val in freq.items():\n",
    "\n",
    "            print (str(key) + ':' + str(val))\n",
    "\n",
    "        freq.plot(30, cumulative=False)\n",
    "        \n",
    "        self.words_tagged()\n",
    "        \n",
    "    def remove_stop_words(self):\n",
    "        \"\"\"Stop Words\"\"\"\n",
    "    \n",
    "        clean_tokens = self.tokens[:]\n",
    "\n",
    "        sr = stopwords.words('english')\n",
    "\n",
    "        for token in self.tokens:\n",
    "\n",
    "            if token in stopwords.words('english'):\n",
    "\n",
    "                clean_tokens.remove(token)\n",
    "        self.tokens=clean_tokens\n",
    "        self.sorted_tokens=sorted(set(self.tokens))\n",
    "        num_words=len(self.tokens)\n",
    "        print(f'Tokens: {self.tokens}\\n')\n",
    "        print(f'Tokens (sorted):{self.sorted_tokens}\\n')\n",
    "        print(f'Word count following stop words: {num_words}\\n')\n",
    "        self.word_freq()\n",
    "    \n",
    "    \n",
    "    def words_tagged(self):\n",
    "        \"\"\"Word Tagging\"\"\"\n",
    "    \n",
    "        self.tagged = nltk.pos_tag(self.tokens)\n",
    "#         tagged[0:6]\n",
    "\n",
    "        print(f'Word Tags:{self.tagged}\\n')\n",
    "    \n",
    "        self.Named_Entity_Recognition()\n",
    "    \n",
    "    \n",
    "    def Named_Entity_Recognition(self):\n",
    "        \"\"\"Named Entity Recognition\"\"\"\n",
    "        \n",
    "        self.NER = ne_chunk(self.tagged)\n",
    "        print(f'Named Entity Recognition:\\n{self.NER}')\n",
    "    \n",
    "    def lexical_diversity(self):\n",
    "        \"\"\"Percentage of word occurance\"\"\"\n",
    "        \n",
    "        lex_div=len(set(self.tokens))/len(self.tokens)\n",
    "        print(f'Lexical Diversity:{(lex_div)*100}\\n')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping Websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Web_Scrapping():\n",
    "    \"\"\"Web Scrapping\"\"\"\n",
    "    \n",
    "    def __init__(self,site):\n",
    "        self.site=site\n",
    "        self.web_scrape_html()\n",
    "      \n",
    "    def web_scrape_html(self):\n",
    "        \"\"\"Retrieve site html\"\"\"\n",
    "    \n",
    "        import urllib.request\n",
    "\n",
    "        response = urllib.request.urlopen(self.site)\n",
    "\n",
    "        self.html = response.read()\n",
    "\n",
    "    #     print (html)\n",
    "\n",
    "        self.web_scrape_text()\n",
    "    \n",
    "    def web_scrape_text(self):\n",
    "        \"\"\"Return text from the site\"\"\"\n",
    "        \n",
    "        from bs4 import BeautifulSoup\n",
    "\n",
    "        soup = BeautifulSoup(self.html,\"html5lib\")\n",
    "\n",
    "        self.text = soup.get_text(strip=True)\n",
    "        \n",
    "#       print(self.text)\n",
    "\n",
    "        self.analysis_of_site_text()\n",
    "    \n",
    "    def analysis_of_site_text(self):\n",
    "        \"\"\"Call NLP Analysis class\"\"\"\n",
    "        analysis=NLP_My_Words(self.text)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Wordnet_Stuff():\n",
    "    \n",
    "    def __init__(self,word):\n",
    "        self.word=word\n",
    "        print(f'The word is:{self.word}\\n')\n",
    "        self.definitions()\n",
    "        self.syn_lem()\n",
    "        self.synonym()\n",
    "    def definitions(self):\n",
    "        \"\"\"Definition of the word\"\"\"\n",
    "        from nltk.corpus import wordnet\n",
    "\n",
    "        syn = wordnet.synsets(self.word)\n",
    "        print(f'Definition:{syn[0].definition()}\\n')\n",
    "        \n",
    "    def syn_lem(self):\n",
    "        \n",
    "        from nltk.corpus import wordnet\n",
    "\n",
    "        synonyms = []\n",
    "        for syn in wordnet.synsets(self.word):\n",
    "\n",
    "            for lemma in syn.lemmas():\n",
    "\n",
    "                synonyms.append(lemma.name())\n",
    "\n",
    "        print(f'sny_lem:{synonyms}\\n')\n",
    "        \n",
    "    def synonym(self):\n",
    "    \n",
    "        from nltk.corpus import wordnet\n",
    "\n",
    "        syn = wordnet.synsets(self.word)\n",
    "\n",
    "        print(f'syn Definition:{syn[0].definition()}\\n')\n",
    "\n",
    "        print(f'syn Examples:{syn[0].examples()}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sentiment():\n",
    "    \"\"\"Conduct sentiment scores\"\"\"\n",
    "    \n",
    "    def __init__(self,text):\n",
    "        self.text=text\n",
    "    \n",
    "    def call_sentenance_tokens(self):\n",
    "        \"\"\"option\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def perform_sentiment(self):\n",
    "        \n",
    "        TextBlob(text).sentiment\n",
    "        \n",
    "        pol=lambda x: TextBlob(text).sentiment.polarity\n",
    "        subj=lambda x: TextBlob(text).sentiment.subjectivity\n",
    "        \n",
    "        the_polority=pol(x)\n",
    "        \n",
    "        the_subjectivity=subj(x)\n",
    "        \n",
    "        y=f'Polority:{the_polarity}\\nSubjectivity:{subj}'\n",
    "        \n",
    "        print(y)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def url_to_transcript(url):\n",
    "    \"\"\"Returns specific text data from a url\"\"\"\n",
    "    page=requests.get(url).text\n",
    "    soup=BeautifulSoup(page,\"html5lib\")\n",
    "    text=[p.text for p in soup.find(class_=\"shortDescription-col\").find_all('p')]\n",
    "    print(url)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_url_text(url):\n",
    "    \n",
    "    from urllib import request\n",
    "    \n",
    "    response = request.urlopen(url)\n",
    "    raw = response.read().decode('utf8')\n",
    "    \n",
    "    print(f'Length of the text file:{len(raw)}\\n')\n",
    "    \n",
    "    return raw[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.startfile('C:/Users/Crystal/Desktop/sidebars/nlp_text_dump.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base='C:/Users/Crystal/Desktop/sidebars/nlp_text_dump.txt'\n",
    "other='C:/Users/Crystal/Desktop/sidebars/Dowd Jimmie - 09022019.txt'\n",
    "f = open(base)\n",
    "text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w=NLP_My_Words(text,'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w.nltk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w.nltk_text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "w.nltk_text.concordance('park')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "w.nltk_text.similar('poop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "w.nltk_text.common_contexts(['dog','jim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w.nltk_text.dispersion_plot(['jim','dog','walked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w.nltk_text.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w.lexical_diversity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "site='https://www.oreilly.com/online-learning/enterprise.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s=Web_Scrapping(site=site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_study=Wordnet_Stuff('Natural')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_analy=Sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TextBlob(text).sentiment\n",
    "\n",
    "pol=lambda x: TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pol(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "url_to_transcript(site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "raw=read_url_text(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "z=NLP_My_Words(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q=['natural', 'natural', 'cancel', 'natural', 'natural', 'natural', 'natural', 'natural', 'natural', 'natural', 'instinctive', 'natural', 'raw', 'rude', 'natural', 'natural', 'born', 'innate', 'lifelike', 'natural']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(set(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
