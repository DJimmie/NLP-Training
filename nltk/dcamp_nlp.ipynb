{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK PROCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os \n",
    "import sys\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "from rich.console import Console\n",
    "console=Console(record=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(re.match('Jim','JimDowd'),style=\"bold black on white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/t1.txt\") as f:   \n",
    "    text = f.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.hq.nasa.gov/alsj/LM03_Apollo_Spacecraft_AS1-6.pdf\"\n",
    "# # html = urlopen(url).read()\n",
    "\n",
    "# html= requests.get(url)\n",
    "\n",
    "\n",
    "# text = BeautifulSoup(html.text, 'html.parser').get_text()\n",
    "# text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence tokenizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=sent_tokenize(text)\n",
    "console.print(f'Number of sentences:{len(sentences)}')\n",
    "\n",
    "console.print(sentences,style=\"bold blue on white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=word_tokenize(text)\n",
    "console.print(f'Number of words: {len(words)}')\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "# Find the frequency\n",
    "fdist= FreqDist(words)\n",
    "\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the frequency graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fdist.plot(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words_no_punc=[w for w in words if w.isalpha()==True]\n",
    "print(words_no_punc[:50])\n",
    "print(len(words_no_punc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting graph without punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist=FreqDist(words_no_punc)\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.plot(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "list_of_stopwords=stopwords.words(\"english\")\n",
    "\n",
    "print(list_of_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words=[w for w in words_no_punc if w not in list_of_stopwords]\n",
    "\n",
    "print(clean_words)\n",
    "\n",
    "print(len(clean_words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist=FreqDist(clean_words)\n",
    "\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.plot(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud=WordCloud().generate(text)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wordcloud)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words_joined=' '.join(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud=WordCloud().generate(clean_words_joined)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wordcloud)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter=PorterStemmer()\n",
    "\n",
    "word_list=['Programming','Programmers','Programmable','orbiter','electrical','studies','leaves','plays','am','is','were']\n",
    "\n",
    "stemmed_words=[porter.stem(w) for w in word_list]\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "pos_list=['v','n','a','r']\n",
    "print(f'word_list--->{word_list}')\n",
    "for i in pos_list: \n",
    "    lemmatized_words=[lemmatizer.lemmatize(w,pos=i) for w in word_list]\n",
    "\n",
    "    print(f' POS:{i}\\n{lemmatized_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging (PoS tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag=nltk.pos_tag(word_list)\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence='The three-person EO-3 crew docked with Salyut 7 on 9 February, 1984, and entered the darkened station carrying flashlights.'\n",
    "\n",
    "# sentence=\"\"\"Oversee and direct daily test activities at the Cameron Technology Development Center. \n",
    "# Lead test lab projects for Drilling and Production systems, R&D testing, product development, product design qualification, performance verification, \n",
    "# and product testing/demonstrations. Manage cross-functional team of engineers and technicians for 200+ annual test plans.\"\"\"\n",
    "\n",
    "sentence=\"\"\"The LM while in powered descent is a cross between a helicopter and a spacecraft.\n",
    "Jim Dowd will pilot the spacecraft.\"\"\"\n",
    "\n",
    "tokenized_words=word_tokenize(sentence)\n",
    "\n",
    "tagged_words=nltk.pos_tag(tokenized_words)\n",
    "\n",
    "tagged_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar=\"NP : {<DT>?<JJ>*<NN>} \"\n",
    "\n",
    "parser=nltk.RegexpParser(grammar)\n",
    "\n",
    "output= parser.parse(tagged_words)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar=r\"\"\"NP : {<.*>+}\n",
    "}<JJ>+{\"\"\"\n",
    "parser=nltk.RegexpParser(grammar)\n",
    "\n",
    "output= parser.parse(tagged_words)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"\"\"Jim Dowd served in the Mission Control Center as the System Engineering & Integration Office (SE&I) Representative for the Mission Evaluation Room (MER) console.\"\"\"\n",
    "\n",
    "tokenized_words=word_tokenize(sentence)\n",
    "\n",
    "tagged_words=nltk.pos_tag(tokenized_words)\n",
    "N_E_R=nltk.ne_chunk(tagged_words,binary=False)\n",
    "\n",
    "print(N_E_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_E_R.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for words in wordnet.synsets('car'):\n",
    "    print(f'{words.name()}\\n{words.definition()}\\n{words.examples()}\\n')\n",
    "\n",
    "    for lemma in words.lemmas():\n",
    "        print(lemma)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Hypernyms: Hypernyms gives us a more abstract term for a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word=wordnet.synsets('rocket')[0]\n",
    "\n",
    "print(word.hypernyms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyponyms: Hyponyms gives us a more specific term for a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word=wordnet.synsets('rocket')[0]\n",
    "\n",
    "print(word.hyponyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[word.hyponyms()[i].name() for i in range(len(word.hyponyms()))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a name only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word=wordnet.synsets('rocket')[0]\n",
    "print(word.lemmas()[0].name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms=[]\n",
    "\n",
    "for words in wordnet.synsets('rocket'):\n",
    "    for lemma in words.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antonyms=[]\n",
    "\n",
    "for words in wordnet.synsets('Natural'):\n",
    "    for lemma in words.lemmas():\n",
    "        if lemma.antonyms():\n",
    "            antonyms.append(lemma.antonyms()[0].name())\n",
    "antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonyms and antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms=[]\n",
    "antonyms=[]\n",
    "for words in wordnet.synsets('Natural'):\n",
    "    for lemma in words.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "        if lemma.antonyms():\n",
    "            antonyms.append(lemma.antonyms()[0].name())\n",
    "\n",
    "\n",
    "print(f'synonyms:\\n{synonyms}\\n')\n",
    "print(f'antonyms:\\n{antonyms}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Finding the similarity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1=wordnet.synsets('ship','n')[0]\n",
    "\n",
    "word2=wordnet.synsets('dog','n')[0]\n",
    "\n",
    "word1.wup_similarity(word2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# text=text.lower()\n",
    "# Counter(word_tokenize(text))\n",
    "\n",
    "# Counter(text).most_common(2)\n",
    "# Counter(word_tokenize(text)).most_common(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# sentences=['Jim is a person', 'Jim likes to fly.', 'Jim wants to walk his dog.']\n",
    "\n",
    "cv=CountVectorizer(stop_words='english',ngram_range=(1,1))\n",
    "\n",
    "B_O_W= cv.fit_transform(sentences).toarray()\n",
    "sum_words = B_O_W.sum(axis=0)\n",
    "words_freq = [(word, sum_words[idx]) for word, idx in cv.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1],reverse=True)\n",
    "\n",
    "\n",
    "print(cv.vocabulary_)\n",
    "print(cv.get_feature_names())\n",
    "print(B_O_W)\n",
    "print(f'BOW shape: {B_O_W.shape}')\n",
    "print(sum_words)\n",
    "print(words_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=20\n",
    "top_words = words_freq[0:n]\n",
    "top_df = pd.DataFrame(top_words)\n",
    "top_df.columns=[\"Word\", \"Freq\"]\n",
    "\n",
    "top_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "g = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer=TfidfVectorizer(norm=None,stop_words='english')\n",
    "\n",
    "X=vectorizer.fit_transform(sentences).toarray()\n",
    "\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Example of Python client calling Knowledge Graph Search API.\"\"\"\n",
    "# from __future__ import print_function\n",
    "# import json\n",
    "# import urllib\n",
    "\n",
    "# api_key = open('.api_key').read()\n",
    "# query = 'Taylor Swift'\n",
    "# service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
    "# params = {\n",
    "#     'query': query,\n",
    "#     'limit': 10,\n",
    "#     'indent': True,\n",
    "#     'key': api_key,\n",
    "# }\n",
    "# url = service_url + '?' + urllib.urlencode(params)\n",
    "# response = json.loads(urllib.urlopen(url).read())\n",
    "# for element in response['itemListElement']:\n",
    "#   print(element['result']['name'] + ' (' + str(element['resultScore']) + ')')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a7267efbf6fadf323e6919ae49483da79d63800eb323ed7cd34ea2e36e2688e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
